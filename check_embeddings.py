#!/usr/bin/env python3
"""
–ü—Ä–æ–≤–µ—Ä–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –º–æ–¥–µ–ª–∏
"""

import torch
from torch.utils.data import DataLoader
from pathlib import Path
import numpy as np
import argparse
import json

from eegclip.data import ThingsEEGDataset, create_subject_splits, collate_fn
from eegclip.models import EEGCLIPModel
from eegclip.utils import load_checkpoint, get_device, load_config
from eegclip.metrics import compute_retrieval_metrics


def check_embeddings(checkpoint_path=None, config_path=None, data_root="data", n_classes=10, device_str="cuda:0"):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    
    print("=" * 70)
    print("üîç –ü–†–û–í–ï–†–ö–ê –≠–ú–ë–ï–î–î–ò–ù–ì–û–í –ú–û–î–ï–õ–ò")
    print("=" * 70)
    
    device = get_device(device_str)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞, –µ—Å–ª–∏ –µ—Å—Ç—å
    config = None
    if checkpoint_path and Path(checkpoint_path).exists():
        config_dir = Path(checkpoint_path).parent
        config_path = config_dir / "config.json"
        if config_path.exists():
            config = load_config(config_path)
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {config_path}")
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –∏–ª–∏ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ
    if config:
        use_features = config.get('use_features', False)
        n_features = config.get('n_features', 155)  # 19 —Å—Ä–µ–¥–Ω–∏—Ö + 136 –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –¥–ª—è 17 –∫–∞–Ω–∞–ª–æ–≤
        eeg_d_model = config.get('eeg_d_model', 256)
        eeg_layers = config.get('eeg_layers', 4)
        eeg_hidden = config.get('eeg_hidden', 512)
        vision_encoder = config.get('vision_encoder', 'openclip_vit_b32')
        freeze_vision = config.get('freeze_vision', True)
        proj_dim = config.get('proj_dim', 512)
        proj_hidden = config.get('proj_hidden', 1024)
        dropout = config.get('dropout', 0.1)
        temporal_pool = config.get('temporal_pool', 'cls')
    else:
        use_features = False
        n_features = 155
        eeg_d_model = 256
        eeg_layers = 4
        eeg_hidden = 512
        vision_encoder = 'openclip_vit_b32'
        freeze_vision = True
        proj_dim = 512
        proj_hidden = 1024
        dropout = 0.1
        temporal_pool = 'cls'
    
    # Subject splits
    all_subjects = list(range(1, 11))
    subject_splits = create_subject_splits(
        all_subjects,
        val_ratio=0.1,
        test_ratio=0.1,
        seed=42
    )
    
    # –î–∞—Ç–∞—Å–µ—Ç
    val_dataset = ThingsEEGDataset(
        data_root=data_root,
        n_classes=n_classes,
        split='val',
        subject_splits=subject_splits,
        eeg_len=100,
        fs=500.0,
        preprocess_eeg=False,
        augment=False,
        use_features=use_features
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=8,
        shuffle=False,
        num_workers=0,
        collate_fn=collate_fn
    )
    
    # –ú–æ–¥–µ–ª—å
    model = EEGCLIPModel(
        n_channels=17,
        n_timepoints=100,
        use_features=use_features,
        n_features=n_features,
        eeg_d_model=eeg_d_model,
        eeg_layers=eeg_layers,
        eeg_hidden=eeg_hidden,
        vision_encoder=vision_encoder,
        freeze_vision=freeze_vision,
        proj_dim=proj_dim,
        proj_hidden=proj_hidden,
        dropout=dropout,
        temporal_pool=temporal_pool
    ).to(device)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç, –µ—Å–ª–∏ –µ—Å—Ç—å
    if checkpoint_path and Path(checkpoint_path).exists():
        print(f"üìù –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–∞: {checkpoint_path}")
        load_checkpoint(Path(checkpoint_path), model)
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω —á–µ–∫–ø–æ–∏–Ω—Ç: {checkpoint_path}")
    else:
        print("üìù –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å —Å –Ω–∞—á–∞–ª—å–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏")
    
    model.eval()
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
    all_eeg_emb = []
    all_img_emb = []
    all_class_indices = []
    
    print(f"\nüìä –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    with torch.no_grad():
        for batch in val_loader:
            eeg = batch['eeg'].to(device)
            image = batch['image'].to(device)
            eeg_emb, img_emb = model(eeg, image)
            all_eeg_emb.append(eeg_emb.cpu())
            all_img_emb.append(img_emb.cpu())
            all_class_indices.extend(batch['class_idx'].tolist())
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    eeg_emb = torch.cat(all_eeg_emb, dim=0)
    img_emb = torch.cat(all_img_emb, dim=0)
    
    print(f"   –í—Å–µ–≥–æ –æ–±—Ä–∞–∑—Ü–æ–≤: {eeg_emb.shape[0]}")
    print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤: {len(set(all_class_indices))}")
    
    # –î–ª—è –∞–Ω–∞–ª–∏–∑–∞ –±–µ—Ä–µ–º –æ–¥–∏–Ω –±–∞—Ç—á
    batch = next(iter(val_loader))
    eeg_sample = batch['eeg'].to(device)
    image_sample = batch['image'].to(device)
    
    print(f"\nüìä –ü—Ä–∏–º–µ—Ä –±–∞—Ç—á–∞:")
    print(f"   EEG shape: {eeg_sample.shape}")
    print(f"   Image shape: {image_sample.shape}")
    print(f"   Class indices: {batch['class_idx'].tolist()}")
    
    print(f"\nüìä –≠–º–±–µ–¥–¥–∏–Ω–≥–∏:")
    print(f"   EEG embeddings shape: {eeg_emb.shape}")
    print(f"   Image embeddings shape: {img_emb.shape}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é
    eeg_norms = torch.norm(eeg_emb, dim=1)
    img_norms = torch.norm(img_emb, dim=1)
    
    print(f"\nüìè L2 –Ω–æ—Ä–º—ã:")
    print(f"   EEG norms: min={eeg_norms.min():.4f}, max={eeg_norms.max():.4f}, mean={eeg_norms.mean():.4f}")
    print(f"   Image norms: min={img_norms.min():.4f}, max={img_norms.max():.4f}, mean={img_norms.mean():.4f}")
    
    # –ú–∞—Ç—Ä–∏—Ü–∞ —Å—Ö–æ–¥—Å—Ç–≤–∞ (–ø–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π)
    eeg_emb_device = eeg_emb.to(device)
    img_emb_device = img_emb.to(device)
    similarity = eeg_emb_device @ img_emb_device.T
    print(f"\nüìä –ú–∞—Ç—Ä–∏—Ü–∞ —Å—Ö–æ–¥—Å—Ç–≤–∞:")
    print(f"   Shape: {similarity.shape}")
    print(f"   –î–∏–∞–≥–æ–Ω–∞–ª—å (–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø–∞—Ä—ã): {torch.diag(similarity).tolist()}")
    print(f"   –î–∏–∞–≥–æ–Ω–∞–ª—å mean: {torch.diag(similarity).mean():.4f}")
    print(f"   –î–∏–∞–≥–æ–Ω–∞–ª—å std: {torch.diag(similarity).std():.4f}")
    print(f"   –í–Ω–µ –¥–∏–∞–≥–æ–Ω–∞–ª–∏ mean: {similarity[~torch.eye(similarity.shape[0], dtype=bool)].mean():.4f}")
    print(f"   –í–Ω–µ –¥–∏–∞–≥–æ–Ω–∞–ª–∏ std: {similarity[~torch.eye(similarity.shape[0], dtype=bool)].std():.4f}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ª–∏ –º–æ–¥–µ–ª—å —Ä–∞–∑–ª–∏—á–∞–µ—Ç –ø–∞—Ä—ã
    diag_similarity = torch.diag(similarity)
    off_diag_mean = similarity[~torch.eye(similarity.shape[0], dtype=bool)].mean()
    
    print(f"\nüîç –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–ª–∏—á–∏–º–æ—Å—Ç–∏:")
    print(f"   –°—Ä–µ–¥–Ω–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø–∞—Ä: {diag_similarity.mean():.4f}")
    print(f"   –°—Ä–µ–¥–Ω–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø–∞—Ä: {off_diag_mean:.4f}")
    print(f"   –†–∞–∑–Ω–∏—Ü–∞: {diag_similarity.mean() - off_diag_mean:.4f}")
    
    if diag_similarity.mean() > off_diag_mean:
        print("   ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø–∞—Ä—ã –∏–º–µ—é—Ç –±–æ–ª—å—à–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ")
    else:
        print("   ‚ùå –ü–†–û–ë–õ–ï–ú–ê: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø–∞—Ä—ã –ù–ï –∏–º–µ—é—Ç –±–æ–ª—å—à–µ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞!")
    
    # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
    print(f"\nüìä –ê–Ω–∞–ª–∏–∑ –Ω–∞ –≤—Å–µ—Ö {eeg_emb.shape[0]} –æ–±—Ä–∞–∑—Ü–∞—Ö:")
    metrics = compute_retrieval_metrics(eeg_emb.to(device), img_emb.to(device), k_list=[1, 5, 10])
    
    print(f"\nüìà –ú–µ—Ç—Ä–∏–∫–∏ retrieval:")
    for key, value in metrics.items():
        print(f"   {key}: {value:.4f}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º logit_scale
    logit_scale_param = model.get_logit_scale_param()  # –°–∞–º –ø–∞—Ä–∞–º–µ—Ç—Ä
    temperature = model.get_logit_scale()  # –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (exp —Å –∫–ª–∏–ø–ø–∏–Ω–≥–æ–º)
    print(f"\nüå°Ô∏è  Logit scale (—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞):")
    print(f"   logit_scale (–ø–∞—Ä–∞–º–µ—Ç—Ä): {logit_scale_param.item():.4f}")
    print(f"   temperature: {temperature.item():.4f}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∫–∞–∫ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≤–ª–∏—è–µ—Ç –Ω–∞ —Å—Ö–æ–¥—Å—Ç–≤–æ
    scaled_similarity = temperature * similarity
    print(f"\nüìä –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ —Å—Ö–æ–¥—Å—Ç–≤–∞ (—Å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–æ–π):")
    print(f"   –î–∏–∞–≥–æ–Ω–∞–ª—å mean: {torch.diag(scaled_similarity).mean():.4f}")
    print(f"   –í–Ω–µ –¥–∏–∞–≥–æ–Ω–∞–ª–∏ mean: {scaled_similarity[~torch.eye(scaled_similarity.shape[0], dtype=bool)].mean():.4f}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    pred_eeg2img = similarity.argmax(dim=1)
    pred_img2eeg = similarity.argmax(dim=0)
    labels = torch.arange(similarity.shape[0], device=device)
    
    eeg2img_acc = (pred_eeg2img == labels).float().mean().item()
    img2eeg_acc = (pred_img2eeg == labels).float().mean().item()
    
    print(f"\nüéØ –¢–æ—á–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
    print(f"   EEG‚ÜíImage: {eeg2img_acc:.4f} ({eeg2img_acc*100:.2f}%)")
    print(f"   Image‚ÜíEEG: {img2eeg_acc:.4f} ({img2eeg_acc*100:.2f}%)")
    print(f"   Baseline (—Å–ª—É—á–∞–π–Ω–æ–µ): {1.0/similarity.shape[0]:.4f} ({100.0/similarity.shape[0]:.2f}%)")
    
    return metrics


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Check embeddings of EEG-CLIP model')
    parser.add_argument('--checkpoint_path', type=str, default=None,
                       help='Path to checkpoint file')
    parser.add_argument('--config_path', type=str, default=None,
                       help='Path to config.json (optional, auto-detected from checkpoint dir)')
    parser.add_argument('--data_root', type=str, default='data',
                       help='Root directory with data')
    parser.add_argument('--n_classes', type=int, default=10,
                       help='Number of classes')
    parser.add_argument('--device', type=str, default='cuda:0',
                       help='Device to use (cuda:0, cpu, etc.)')
    
    args = parser.parse_args()
    
    # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω —á–µ–∫–ø–æ–∏–Ω—Ç, –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –µ–≥–æ
    if args.checkpoint_path:
        metrics = check_embeddings(
            checkpoint_path=args.checkpoint_path,
            config_path=args.config_path,
            data_root=args.data_root,
            n_classes=args.n_classes,
            device_str=args.device
        )
    else:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–æ–¥–µ–ª—å —Å –Ω–∞—á–∞–ª—å–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏
        print("\n" + "="*70)
        print("1. –ü–†–û–í–ï–†–ö–ê –ú–û–î–ï–õ–ò –° –ù–ê–ß–ê–õ–¨–ù–´–ú–ò –í–ï–°–ê–ú–ò")
        print("="*70)
        metrics_init = check_embeddings(
            data_root=args.data_root,
            n_classes=args.n_classes,
            device_str=args.device
        )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å (–µ—Å–ª–∏ –µ—Å—Ç—å –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç)
        checkpoint_path = "checkpoints_test/best.pt"
        if Path(checkpoint_path).exists():
            print("\n" + "="*70)
            print("2. –ü–†–û–í–ï–†–ö–ê –û–ë–£–ß–ï–ù–ù–û–ô –ú–û–î–ï–õ–ò")
            print("="*70)
            metrics_trained = check_embeddings(
                checkpoint_path=checkpoint_path,
                data_root=args.data_root,
                n_classes=args.n_classes,
                device_str=args.device
            )
            
            print("\n" + "="*70)
            print("üìä –°–†–ê–í–ù–ï–ù–ò–ï")
            print("="*70)
            print(f"–ù–∞—á–∞–ª—å–Ω–∞—è Recall@1: {metrics_init['eeg2img_recall@1']:.4f}")
            print(f"–û–±—É—á–µ–Ω–Ω–∞—è Recall@1: {metrics_trained['eeg2img_recall@1']:.4f}")
            print(f"–£–ª—É—á—à–µ–Ω–∏–µ: {metrics_trained['eeg2img_recall@1'] - metrics_init['eeg2img_recall@1']:.4f}")
        else:
            print(f"\n‚ö†Ô∏è  –ß–µ–∫–ø–æ–∏–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {checkpoint_path}")

